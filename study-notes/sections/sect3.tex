\section{Duality Theory}
\label{sect:duality}
\begin{enumerate}
\item In our context here, \emph{duality} refers to the connection between two
LP problems, which are called \emph{primal} and \emph{dual} problems
respectively. The LP problems we have been working with so far can all be
treated as \emph{primal} problems. Duality theory provides us another point of
view on those LP problems through their duals, thereby giving us:
\begin{itemize}
\item new tools to analyze those LP problems by \emph{weak duality} and
\emph{strong duality} (\Cref{thm:weak-duality,thm:strong-duality}),
\item new optimality conditions like \emph{complementary slackness} (\labelcref{it:comp-slackness}),
\item new algorithms for solving LPs like \emph{dual simplex method} (\Cref{subsect:dual-simplex-method}).
\end{itemize}
Indeed, for general optimization problems, duality plays an important role
also.
\end{enumerate}
\subsection{Primal and Dual Problems}
\begin{enumerate}
\item \textbf{Definitions of primal and dual problems.} Let \(A\) be a matrix
with rows \(\vect{a}_1^{T},\dotsc,\vect{a}_m^{T}\) and columns
\(\vect{A}_1,\dotsc,\vect{A}_n\).
A \defn{primal} problem
refers to a general LP problem of minimization here:
\begin{align*}
\text{min}\quad&\vect{c}^{T}\vect{x} \\
\text{s.t.}\quad&\vect{a}_j^{T}\vect{x}\ge b_j \text{ for all \(j\in M_1\),} \\
\quad&\vect{a}_j^{T}\vect{x}\le b_j \text{ for all \(j\in M_2\),} \\
\quad&\vect{a}_j^{T}\vect{x}=b_j \text{ for all \(j\in M_3\),} \\
\quad&x_i\ge 0\text{ for all \(i\in N_1\),} \\
\quad&x_i\le 0\text{ for all \(i\in N_2\),} \\
\quad&x_i\text{ free}\text{ for all \(i\in N_3\).}
\end{align*}
Its \defn{dual} is then defined as the following LP problem of maximization:
\begin{align*}
\text{max}\quad&\vect{b}^{T}\vect{y} \\
\text{s.t.}\quad&y_j\ge 0 \text{ for all \(j\in M_1\),} \\
\quad&y_j\le 0 \text{ for all \(j\in M_2\),} \\
\quad&y_j\text{ free} \text{ for all \(j\in M_3\),} \\
\quad&\vect{A}_i^{T}\vect{y}\le c_i\text{ for all \(i\in N_1\),} \\
\quad&\vect{A}_i^{T}\vect{y}\ge c_i\text{ for all \(i\in N_2\),} \\
\quad&\vect{A}_i^{T}\vect{y}=c_i\text{ for all \(i\in N_3\).}
\end{align*}
The following table tells us the ``rule'' for ``translating'' between primal and
dual problems:
\begin{center}
\begin{tabular}{cccc}
\toprule
Primal&minimize&maximize&Dual \\
\midrule
\multirow{3}{*}{constraints}&\(\ge b_j\)&\(\ge 0\)&\multirow{3}{*}{variables} \\
&\(\le b_j\)&\(\le 0\)& \\
&\(=b_j\)&free& \\
\multirow{3}{*}{variables}&\(\ge 0\)&\(\rc{\le} c_i\)&\multirow{3}{*}{constraints} \\
&\(\le 0\)&\(\rc{\ge} c_i\)& \\
&free&\(=c_i\)& \\
\bottomrule
\end{tabular}
\end{center}
\item\label{it:dual-dual-primal} \textbf{The dual of the dual is the primal.}
One nice property of the dual can be summarized in one sentence: \emph{The dual
of the dual is the primal.} This means that taking dual twice would give you
back the original LP problem. To see why this holds, consider the following:
\begin{enumerate}[label={(\arabic*)}]
\item Start with a primal:
\begin{align*}
\text{min}\quad&\vect{c}^{T}\vect{x} \\
\text{s.t.}\quad&\vect{a}_j^{T}\vect{x}\ge b_j \text{ for all \(j\in M_1\),} \\
\quad&\vect{a}_j^{T}\vect{x}\le b_j \text{ for all \(j\in M_2\),} \\
\quad&\vect{a}_j^{T}\vect{x}=b_j \text{ for all \(j\in M_3\),} \\
\quad&x_i\ge 0\text{ for all \(i\in N_1\),} \\
\quad&x_i\le 0\text{ for all \(i\in N_2\),} \\
\quad&x_i\text{ free}\text{ for all \(i\in N_3\).}
\end{align*}
\item Its dual is:
\begin{align*}
\text{max}\quad&\vect{b}^{T}\vect{y} \\
\text{s.t.}\quad&y_j\ge 0 \text{ for all \(j\in M_1\),} \\
\quad&y_j\le 0 \text{ for all \(j\in M_2\),} \\
\quad&y_j\text{ free} \text{ for all \(j\in M_3\),} \\
\quad&\vect{A}_i^{T}\vect{y}\le c_i\text{ for all \(i\in N_1\),} \\
\quad&\vect{A}_i^{T}\vect{y}\ge c_i\text{ for all \(i\in N_2\),} \\
\quad&\vect{A}_i^{T}\vect{y}=c_i\text{ for all \(i\in N_3\).}
\end{align*}
\item To get the ``dual of the dual'', we need to first convert the dual
problem above to a minimization problem as follows:
\begin{align*}
\text{min}\quad&(-\vect{b})^{T}\vect{y} \\
\text{s.t.}\quad&(-\vect{A}_i)^{T}\vect{y}\ge -c_i\text{ for all \(i\in N_1\),} \\
\quad&(-\vect{A}_i)^{T}\vect{y}\le -c_i\text{ for all \(i\in N_2\),} \\
\quad&(-\vect{A}_i)^{T}\vect{y}=-c_i\text{ for all \(i\in N_3\),} \\
\quad&y_j\ge 0 \text{ for all \(j\in M_1\),} \\
\quad&y_j\le 0 \text{ for all \(j\in M_2\),} \\
\quad&y_j\text{ free} \text{ for all \(j\in M_3\).}
\end{align*}
\item Viewing \(-\vect{A}_i\)'s as the rows of \(-A^{T}\) and \(-\vect{a}_j\)'s as
the columns of \(-A^{T}\), the dual of this problem is then:
\begin{align*}
\text{max}\quad&(-\vect{c})^{T}\vect{x} \\
\text{s.t.}\quad&x_i\ge 0\text{ for all \(i\in N_1\),} \\
\quad&x_i\le 0\text{ for all \(i\in N_2\),} \\
\quad&x_i\text{ free}\text{ for all \(i\in N_3\),} \\
\quad&(-\vect{a}_j)^{T}\vect{x}\le -b_j \text{ for all \(j\in M_1\),} \\
\quad&(-\vect{a}_j)^{T}\vect{x}\ge -b_j \text{ for all \(j\in M_2\),} \\
\quad&(-\vect{a}_j)^{T}\vect{x}=-b_j\text{ for all \(j\in M_3\).}
\end{align*}
\item Converting it back to a minimization problem yields the original primal:
\begin{align*}
\text{min}\quad&\vect{c}^{T}\vect{x} \\
\text{s.t.}\quad&\vect{a}_j^{T}\vect{x}\ge b_j \text{ for all \(j\in M_1\),} \\
\quad&\vect{a}_j^{T}\vect{x}\le b_j \text{ for all \(j\in M_2\),} \\
\quad&\vect{a}_j^{T}\vect{x}=b_j \text{ for all \(j\in M_3\),} \\
\quad&x_i\ge 0\text{ for all \(i\in N_1\),} \\
\quad&x_i\le 0\text{ for all \(i\in N_2\),} \\
\quad&x_i\text{ free}\text{ for all \(i\in N_3\).}
\end{align*}
\end{enumerate}
\item \textbf{Duality for standard form LP problems.} For standard form
LP problems, the expressions of primal and dual can be significantly simplified
as follows:
\begin{align*}
&\text{\underline{Primal}}&&\text{\underline{Dual}} \\
\text{min}\quad&\vect{c}^{T}\vect{x}&\text{max}\quad&\vect{b}^{T}\vect{y} \\
\text{s.t.}\quad&A\vect{x}=\vect{b},&\text{s.t.}\quad&A^{T}\vect{y}\le\vect{c}. \\
\quad&\vect{x}\ge\vect{0}.
\end{align*}
By reducing a general LP problem to an equivalent standard form problem
(through the operations mentioned in \labelcref{it:reduce-to-std-form}), we can
study the duality for the general LP problem through the simpler duality for
standard form LP problem, as the following result suggests:

\begin{theorem}
\label{thm:dual-equiv}
Suppose that we have transformed a general LP problem \(\Pi_1\) to an
equivalent standard form LP problem \(\Pi_2\) (through the operations in
\labelcref{it:reduce-to-std-form}).  Let \(D_1\) and \(D_2\) be the duals of
\(\Pi_1\) and \(\Pi_2\) respectively.  Then, \(D_1\) and \(D_2\) are either
both infeasible or have the same finite optimal value.
\end{theorem}
\begin{pf}
Omitted.
\end{pf}

Based on \Cref{thm:dual-equiv}, we can have the following route to study the
duality of general LP problem \(\Pi_1\) via the duality for standard form
LP problem.
\begin{center}
\begin{tikzpicture}
\node[] () at (0,3) {\(\Pi_1\)};
\node[] () at (0,0) {\(\Pi_2\)};
\node[] () at (6,0) {\(D_2\)};
\node[] () at (6,3) {\(D_1\)};
\draw[-Latex, very thick] (0,2.5) -- (0,0.5) node[midway, left]{transform by \labelcref{it:reduce-to-std-form}};
\draw[-Latex, very thick] (0.5,0) -- (5.5,0) node[midway, below, text width=4cm]{duality for standard form LP problem};
\draw[-Latex, very thick] (6,0.5) -- (6,2.5) node[midway, right]{\Cref{thm:dual-equiv}};
\end{tikzpicture}
\end{center}
\end{enumerate}
\subsection{Duality Theorems}
\label{subsect:duality-thms}
\begin{enumerate}
\item Two major duality theorems that relate the primal and dual problems are
\emph{weak duality} and \emph{strong duality}, which establish \emph{inequality}
and \emph{equality} on the optimal values of the primal and dual problems
respectively.
\item \textbf{Weak duality.} As the following result suggests, the optimal
value of the dual is always less than or equal to the optimal value of the
primal; indeed the same is true for general \emph{objective function value}:
\begin{theorem}[Weak duality]
\label{thm:weak-duality}
If \(\vect{x}\) and \(\vect{y}\) are feasible solutions to the primal and the
dual problems respectively, then \(\vect{b}^{T}\vect{y}\le\vect{c}^{T}\vect{x}\).
\end{theorem}
\begin{pf}
Fix any feasible solution \(\vect{x}\) to the primal and feasible solution
\(\vect{y}\) to the dual. Let \(u_i:=y_i(\vect{a}_i^{T}\vect{x}-b_i)\) and
\(v_j:=(c_j-\vect{A}_j^{T}\vect{y})x_j\). By the primal and dual feasibility,
the signs of \(y_i\) and \(\vect{a}_i^{T}\vect{x}-b_i\) are the same, so do
the signs of \(c_j-\vect{A}_j^{T}\vect{y}\) and \(x_j\). Hence, we must have
\(u_i\ge 0\) for every \(i\) and \(v_j\ge 0\) for every \(j\). This then implies that
\[
\vect{c}^{T}\vect{x}-\vect{b}^{T}\vect{y}
=(\vect{y}^{T}A\vect{x}-\vect{b}^{T}\vect{y})
-(\vect{c}^{T}\vect{x}-\vect{x}^{T}A^{T}\vect{y})
=\sum_{i}^{}u_i+\sum_{j}^{}v_j
\ge 0,
\]
as desired.
\end{pf}

Based on weak duality, we can obtain the following tests for the feasibility of
the primal and dual problems.
\begin{corollary}
\label{cor:prim-dual-feas}
\hfill
\begin{enumerate}
\item If the optimal value of the primal is \(-\infty\), then the dual problem
must be infeasible.
\item If the optimal value of the dual is \(\infty\), then the primal problem
must be infeasible.
\end{enumerate}
\end{corollary}
\begin{pf}
\begin{enumerate}
\item Assume to the contrary the optimal value of the primal is \(-\infty\)
while there exists a feasible solution \(\vect{y}\) to the dual problem. Then,
by weak duality, we have \(\vect{b}^{T}\vect{y}\le\vect{c}^{T}\vect{x}\)
for every feasible solution \(\vect{x}\) to the primal problem, which would
then imply that the objective function for the primal problem is bounded below
(by \(\vect{b}^{T}\vect{y}\in\R\)) on the feasible region, thus the optimal
value of the primal would not be \(-\infty\), contradiction.
\item Similar to (a).
\end{enumerate}
\end{pf}

Another helpful result as a corollary of the weak duality provides an
optimality condition for both primal and dual. Later in
\labelcref{it:comp-slackness}, we will have more discussions on the optimality
condition.
\begin{corollary}
\label{cor:primal-dual-optim}
Let \(\vect{x}\) and \(\vect{y}\) be feasible solutions to the primal and the
dual problems respectively. If \(\vect{c}^{T}\vect{x}=\vect{b}^{T}\vect{y}\),
then \(\vect{x}\) and \(\vect{y}\) are optimal solutions to the primal and the
dual respectively.
\end{corollary}
\begin{pf}
We first show the optimality of \(\vect{x}\) for the primal. Fix any feasible
solution \(\vect{z}\) to the primal. Then, we have
\(\vect{c}^{T}\vect{x}=\vect{b}^{T}\vect{y}\overset{\text{(weak duality)}}{\le}\vect{c}^{T}\vect{z}\),
establishing the optimality of \(\vect{x}\).

Next, we show the optimality of \(\vect{y}\) for the dual. Fix any feasible
solution \(\vect{w}\) to the dual. Then, we have
\(\vect{b}^{T}\vect{y}=\vect{c}^{T}\vect{x}\overset{\text{(weak duality)}}{\ge}\vect{b}^{T}\vect{w}\),
establishing the optimality of \(\vect{y}\).
\end{pf}
\item \textbf{Strong duality.} The next important duality theorem is the
\emph{strong duality}, which is often considered to be the \emph{central
result} for studying duality of LP problems.

\begin{theorem}[Strong duality]
\label{thm:strong-duality}
If a LP problem has an optimal solution, then so does its dual, and the
respective optimal values are equal.
\end{theorem}
\begin{pf}
We first prove the result for the case with standard form primal.
\begin{align*}
&\text{\underline{Primal}}&&\text{\underline{Dual}} \\
\text{min}\quad&\vect{c}^{T}\vect{x}&\text{max}\quad&\vect{b}^{T}\vect{y} \\
\text{s.t.}\quad&A\vect{x}=\vect{b},&\text{s.t.}\quad&A^{T}\vect{y}\le\vect{c}. \\
\quad&\vect{x}\ge\vect{0}.
\end{align*}
Applying the simplex method on the primal problem, at the termination we would
get an optimal basic feasible solution \(\vect{x}\) with an optimal basis
matrix \(B\). Let \(\vect{x}_{B}:=B^{-1}\vect{b}\) denote the vector of basic
variables. Also, we know the reduced costs must all be nonnegative at the
termination, so
\(\bar{\vect{c}}=\vect{c}-A^{T}\vc{(B^{-1})^{T}\vect{c}_{B}}\ge\vect{0}\).

Now we choose \(\vect{y}=\vc{(B^{-1})^{T}\vect{c}_{B}}\). Since
\(\bar{\vect{c}}\ge\vect{0}\), we have \(A^{T}\vect{y}\le\vect{c}\), hence
\(\vect{y}\) is a feasible solution to the dual problem. Also, we have
\(\vect{b}^{T}\vect{y}=\vect{b}^{T}(B^{-1})^{T}\vect{c}_{B}
=\vect{c}_{B}^{T}B^{-1}\vect{b}=\vect{c}_{B}\vect{x}_{B}=\vect{c}^{T}\vect{x}\).
Thus, by \Cref{cor:primal-dual-optim}, \(\vect{y}\) serves as an optimal
solution to the dual problem, and also the respective optimal values are equal
as \(\vect{b}^{T}\vect{y}=\vect{c}^{T}\vect{x}\).

Now, for the general case where the primal \(\Pi_1\) is a general LP problem,
we first transform it to an equivalent standard form LP problem \(\Pi_2\)
(through \labelcref{it:reduce-to-std-form}). Now let \(D_1\) and \(D_2\) be the
duals of \(\Pi_1\) and \(\Pi_2\) respectively. From the special case proven
above, \(\Pi_2\) and \(D_2\) have the same optimal value.

From this we deduce that \(D_2\) must be feasible, so by \Cref{thm:dual-equiv},
\(D_1\) and \(D_2\) would have the same optimal value.  Also, from the
equivalence, \(\Pi_1\) and \(\Pi_2\) have the same optimal value by
\Cref{prp:equiv-optim-same-optim}.  This shows that \(\Pi_1\) and \(D_1\) have
the same optimal value.
\end{pf}
\item\label{it:primal-dual-poss} \textbf{Possibilities for the primal and the dual.} By
\Cref{cor:lp-optimal-exist}, for every LP problem, it falls into exactly one of
the following three categories:
\begin{enumerate}[label={(\arabic*)}]
\item It has an optimal solution, and the optimal value is finite.
\item The optimal value is \(-\infty\) (for minimization) or \(\infty\) (for maximization).
\item The problem is infeasible.
\end{enumerate}
By weak duality (more specifically, \Cref{cor:prim-dual-feas}) and strong
duality, we can deduce all possible combinations of the categories for the
primal and dual problems:
\begin{center}
\begin{tabular}{cccc}
\toprule
\diagbox{Dual}{Primal}&Optimal solution exists&Optimal value is \(-\infty\)&Infeasible \\
\midrule
Optimal solution exists&\cmark{} (equal optimal value)&\xmark&\xmark \\
Optimal value is \(\infty\)&\xmark&\xmark&\cmark \\
Infeasible&\xmark&\cmark&\cmark \\
\bottomrule
\end{tabular}
\end{center}
\begin{note}
Technically the results do not assure that it is possible for both primal and
dual problems to be infeasible. But it is indeed not hard to construct an
example where this is the case, e.g.:
\begin{align*}
&\text{\underline{Primal}}&&\text{\underline{Dual}} \\
\text{min}\quad&x_1+2x_2&\text{max}\quad&y_1+3y_2\\
\text{s.t.}\quad&x_1+x_2=1,&\text{s.t.}\quad&y_1+2y_2=1, \\
\quad&2x_1+2x_2=3.&&y_1+2y_2=2.
\end{align*}
\end{note}
\item\label{it:comp-slackness} \textbf{Complementary slackness.}
\emph{Complementary slackness} provides a necessary and sufficient condition
for \(\vect{x}\in\R^n\) and \(\vect{y}\in\R^m\) to be optimal for the primal and dual
problems respectively. For simplicity, here we shall only focus on the case
with standard form primal:
\begin{theorem}[Optimality condition based on complementary slackness]
\label{thm:optim-comp-slackness}
Consider a primal problem and its dual:
\begin{align*}
&\text{\underline{Primal}}&&\text{\underline{Dual}} \\
\text{min}\quad&\vect{c}^{T}\vect{x}&\text{max}\quad&\vect{b}^{T}\vect{y} \\
\text{s.t.}\quad&A\vect{x}=\vect{b},&\text{s.t.}\quad&A^{T}\vect{y}\le\vect{c}. \\
\quad&\vect{x}\ge\vect{0}.
\end{align*}
The vectors \(\vect{x}\in\R^n\) and \(\vect{y}\in\R^m\) are optimal solutions
to the primal and the dual problems respectively iff
\begin{enumerate}[label={(\arabic*)}]
\item \emph{(primal feasibility)} \(A\vect{x}=\vect{b}\) and \(\vect{x}\ge\vect{0}\).
\item \emph{(dual feasibility)} \(A^{T}\vect{y}\le\vect{c}\).
\item \emph{(complementary slackness)} \((c_i-\vect{A}_i^{T}\vect{y})x_i=0\)
for every \(i=1,\dotsc,n\), where \(\vect{A}_i\) is the \(i\)th column of
\(A\) for every \(i\).
\end{enumerate}
\end{theorem}
\begin{pf}
``\(\Rightarrow\)'': Suppose that \(\vect{x}\in\R^n\) and \(\vect{y}\in\R^m\)
are optimal solutions, hence feasible solutions, to the primal and the dual
respectively.  Then the primal feasibility and dual feasibility are
automatically satisfied, so it remains to show the complementary slackness.

By strong duality, we have \(\vect{c}^{T}\vect{x}=\vect{b}^{T}\vect{y}\), which
implies that
\[
\sum_{i=1}^{n}(c_i-\vect{A}_i^{T}\vect{y})x_i
=(\vect{c}-A^{T}\vect{y})^{T}\vect{x}
=\vect{c}^{T}\vect{x}-\vect{x}^{T}A^{T}\vect{y}
=\vect{c}^{T}\vect{x}-(A\vect{x})^{T}\vect{y}
=\vect{c}^{T}\vect{x}-\vect{b}^{T}\vect{y}
=0.
\]
Since both \(\vect{x}\) and \(\vect{y}\) are feasible, we have
\(\vect{A}_i^{T}\vect{y}\le c_i\) and \(x_i\ge 0\) for every \(i=1,\dotsc,n\),
which suggests that each summand is nonnegative. This then forces that
\((c_i-\vect{A}_i^{T}\vect{y})x_i=0\) for every \(i=1,\dotsc,n\).

``\(\Leftarrow\)'': Suppose that the three conditions hold. Then, like above,
we have
\(\vect{c}^{T}\vect{x}-\vect{b}^{T}\vect{y}=\sum_{i=1}^{n}(c_i-\vect{A}_i^{T}\vect{y})x_i
=0\), thus \(\vect{c}^{T}\vect{x}=\vect{b}^{T}\vect{y}\). Hence, by
\Cref{cor:primal-dual-optim}, \(\vect{x}\) and \(\vect{y}\) are optimal
solutions to the primal and the dual respectively.
\end{pf}

\begin{note}
The condition that \((c_i-\vect{A}_i^{T}\vect{y})x_i=0\) for every
\(i=1,\dotsc,n\) is called \emph{complementary slackness} because it asserts
that for every \(i=1,\dotsc,n\), one of the inequalities \(x_i\ge 0\) (primal)
and \(\vect{A}_i^{T}\vect{y}\le c_i\) (dual) must hold \emph{tightly} without
``slack'' (i.e., the equality holds); otherwise the expression would be greater
than \(0\). In other words, appearance of ``\underline{slackness}'' in one
inequality must be \underline{complemented} by the ``non-slackness'' in
another.
\end{note}
\end{enumerate}
\subsection{Dual Simplex Method}
\label{subsect:dual-simplex-method}
\begin{enumerate}
\item The \emph{dual simplex method} is an alternative algorithm to the simplex
method investigated in \Cref{sect:simplex-method}. Here, we will study how it
works, and how the tableau introduced before helps us carry out the dual
simplex method. Like the simplex method, here we will also focus on standard
form LP problems only.
\item \textbf{Motivation.} To motivate the dual simplex method, we consider the
following lemma.
\begin{lemma}
\label{lma:optim-prim-feas-dual-feas}
Consider a primal problem and its dual:
\begin{align*}
&\text{\underline{Primal}}&&\text{\underline{Dual}} \\
\text{min}\quad&\vect{c}^{T}\vect{x}&\text{max}\quad&\vect{b}^{T}\vect{y} \\
\text{s.t.}\quad&A\vect{x}=\vect{b},&\text{s.t.}\quad&A^{T}\vect{y}\le\vect{c}. \\
\quad&\vect{x}\ge\vect{0}.
\end{align*}
Let \(B\in\R^{m\times m}\) be a basis matrix for the primal problem,
corresponding to a basic feasible solution \(\vect{x}\), with
\(\vect{x}_{B}:=B^{-1}\vect{b}\in\R^n\) being the vector of basic variables.
Let \(\vect{y}_{B}:=(B^{-1})^{T}\vect{c}_{B}\in\R^m\). Then:
\begin{enumerate}
\item The vector \(\vect{y}_{B}\) is a basic solution to the dual problem.
\item The basis matrix \(B\) is optimal if \(\vect{x}_{B}\ge\vect{0}\)
\emph{(primal feasibility)} and \(A^{T}\vect{y}_{B}\le\vect{c}\) \emph{(dual
feasibility)}.
\end{enumerate}
\end{lemma}
\begin{pf}
\begin{enumerate}
\item From the basis matrix \(B\), we know that there are \(m\) basic indices
\(B(1),\dotsc,B(m)\) corresponding to the basic variables. By
\labelcref{it:reduced-cost}, the reduced cost is
\(\bar{c}_{B(i)}=c_{B(i)}-\vect{c}_{B}^{T}B^{-1}\vect{A}_{B(i)}=0\) for every
\(i=1,\dotsc,m\), where \(\vect{A}_{B(1)},\dotsc,\vect{A}_{B(m)}\) are linearly
independent. This implies that
\(\vect{A}_{B(i)}^{T}\vect{y}_{B}=\vect{A}_{B(i)}^{T}(B^{-1})^{T}\vect{c}_{B}
=\vect{c}_{B}^{T}B^{-1}\vect{A}_{B(i)}=c_{B(i)}\) for every \(i=1,\dotsc,m\),
which gives us \(m\) linearly independent active constraints at
\(\vect{y}_{B}\) (for the dual problem).  Thus \(\vect{y}_{B}\) is a basic
solution to the dual problem.
\item The condition \(\vect{x}_{B}\ge\vect{0}\) is precisely the feasibility
requirement for \(B\) to be optimal, so it suffices to establish the optimality
requirement, namely that all the reduced costs are nonnegative (for the primal
problem). Using a similar argument as in (a), we can express the reduced costs
as \(\bar{\vect{c}}=\vect{c}-A^{T}\vect{y}_{B}\). Therefore, the condition
\(A^{T}\vect{y}_{B}\le\vect{c}\) is indeed equivalent to
\(\bar{\vect{c}}\ge\vect{0}\), showing that the optimality requirement is
satisfied.
\end{enumerate}
\end{pf}

\begin{note}
For (b), one can verify that the \emph{complementary slackness} property is
also satisfied by \(\vect{x}\in\R^n\) and \(\vect{y}_{B}\in\R^m\): For every
\(j=1,\dotsc,n-m\), we have \(x_{N(j)}=0\); for every \(i=1,\dotsc,m\), we have
\(c_{B(i)}-\vect{A}_{B(i)}^{T}\vect{y}_{B}\overset{\text{(proof above)}}{=}0\).
Hence, by \Cref{thm:optim-comp-slackness}, we can conclude that
\(\vect{x}\in\R^n\) and \(\vect{y}_{B}\in\R^m\) are optimal solutions to the
primal and the dual problems respectively,.
\end{note}
\item \textbf{Comparing simplex method and dual simplex method.} From the proof
of \Cref{lma:optim-prim-feas-dual-feas}, we see that the nonnegativity of
reduced costs in the primal problem actually corresponds to the dual
feasibility for a basic solution \(\vect{y}_{B}\) to the dual problem. Hence,
we can view the (primal) simplex method as an algorithm that keeps changing the
basis matrix \(B\) such that the \underline{primal} feasibility (for the
basic solution with vector of basic variables being
\(\vect{x}_{B}\)) is maintained, and terminates if the \underline{dual}
feasibility (for the basic solution \(\vect{y}_{B}\)) is achieved.

On the other hand, the dual simplex method can be viewed as an algorithm that
keeps changing the basis matrix \(B\) such that the \underline{dual}
feasibility is maintained, and terminates if the \underline{primal} feasibility
is achieved. This gives us the basic idea of how the dual simplex method works;
let us study how it can actually be carried based on the simplex tableau
introduced before next.

\item \textbf{Procedure for the dual simplex method.}
\begin{enumerate}[label={(\arabic*)}]
\item Start with the simplex tableau associated with a basis matrix such that
\vc{all reduced costs are nonnegative} \emph{(dual feasibility)}.
\item Check the signs of the basic variables \(x_{B(1)},\dotsc,x_{B(m)}\) in
the zeroth \vc{column} of the tableau.
\begin{enumerate}
\item If \(x_{B(i)}\ge 0\) for all \(i\), then the current basic feasible
solution \(\vect{x}\) is optimal (for the primal problem), and the algorithm
terminates \faIcon[regular]{pause-circle}.
\item Otherwise, choose a \(\ell\) such that \(x_{B(\ell)}<0\).
\end{enumerate}
\item Consider the \(\ell\)th row (the \defn{pivot row}) with entries
\(x_{B(\ell)},v_1,\dotsc,v_n\). If no entry of the row is \vc{negative}, then
the optimal value for the dual problem is \(\infty\), which implies the
infeasibility of the primal problem, and then the algorithm terminates.
\item For every \(i\) where \(v_i\vc{<0}\), compute the ratio
\vc{\(\bar{c}_i/|v_i|\)}. Let \(j\) be the index for a \vc{column} that
corresponds to the smallest ratio (the \defn{pivot column}). Then the column
\(\vect{A}_{B(\ell)}\) exits the basis and the column \(\vect{A}_j\) enters the
basis.
\item Add to each row of the tableau a multiple of the pivot row (\(\ell\)th
row) such that \(v_j\) (the \defn{pivot element}, which is the intersection
between the pivot row and the pivot column) becomes \(1\) and all other entries
of the pivot column become \(0\).
\item Repeat (2)-(5) until the algorithm terminates \faIcon[regular]{pause-circle}.
\end{enumerate}
\begin{remark}
\item \emph{(similarity to the simplex method)}  As one can see, this procedure
is quite analogous to the one for the simplex method (see
\labelcref{it:tableau-algo}), just with something ``flipped''.
\item \emph{(usefulness)} Since the dual simplex method often turns out to work
better than the simplex method in practice, it is considered to be
practically useful. Besides, the dual simplex method is usually helpful to deal
with the case where the vector \(\vect{b}\) in the primal problem changes,
since such changes would affect only the zeroth column of the final tableau
obtained by the simplex method, possibly making some entries there negative.
In such case, one cannot directly carry out the (primal) simplex method on that
tableau, but the dual simplex method still works well.
\end{remark}
\end{enumerate}
